{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lv46qrVNzXg-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import numbers\n",
        "\n",
        "\n",
        "\n",
        "OPS = \"+-*\" # avoiding division by default so that Z_n ring works out of the box\n",
        "\n",
        "def merge_two_items(list_of_expressions, rng, operators=OPS):\n",
        "    p = rng.permutation(len(list_of_expressions))\n",
        "    ploe = permuted_list_of_expressions = [list_of_expressions[i] for i in p]\n",
        "    op = operators[rng.randint(len(operators))]\n",
        "    a, b = ploe.pop(), ploe.pop()\n",
        "    ploe.append((a, b, op))\n",
        "    return ploe\n",
        "\n",
        "def create_random_tree_from_list(list_of_items, rng, operators=OPS):\n",
        "    while len(list_of_items) > 1:\n",
        "        list_of_items = merge_two_items(list_of_items, rng, operators)\n",
        "    return list_of_items[0]\n",
        "\n",
        "def render_expression_from_tree(tree,\n",
        "                                render_expr=lambda a, b, op: f\"( {a} {op} {b} )\",\n",
        "                                render_leaf=lambda x: str(x)):\n",
        "\n",
        "    if isinstance(tree, tuple) and len(tree) == 3:\n",
        "        a, b, op = tree\n",
        "        a = render_expression_from_tree(a, render_expr=render_expr, render_leaf=render_leaf)\n",
        "        b = render_expression_from_tree(b, render_expr=render_expr, render_leaf=render_leaf)\n",
        "        return render_expr(a, b, op)\n",
        "    else:\n",
        "\n",
        "        return render_leaf(tree)\n",
        "\n",
        "\n",
        "class ExpressionTreeDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, n_operands=2, n_samples=1000000, operators=\"+-*\",\n",
        "                 n_expressions_per_sample=1, random_state=0):\n",
        "        self.n_operands = n_operands\n",
        "        self.n_samples = n_samples\n",
        "        self.operators = operators\n",
        "        self.n_expressions_per_sample = n_expressions_per_sample\n",
        "        self.random_state = random_state\n",
        "\n",
        "        global_rng = np.random.RandomState(random_state)\n",
        "        self.seeds = global_rng.randint(0, 2**32-1, self.n_samples)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rng = np.random.RandomState(self.seeds[idx])\n",
        "        items = []\n",
        "        numbers = list(range(self.n_operands))\n",
        "        for _ in range(self.n_expressions_per_sample):\n",
        "            tree = create_random_tree_from_list(numbers, rng, self.operators)\n",
        "            items.append(tree)\n",
        "        return items\n",
        "\n",
        "\n",
        "class ModuloArithmeticDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, Zn=7,\n",
        "                 n_operands=2, n_samples=1000000,\n",
        "                 n_items_per_sample=1, operators=\"+-*\",\n",
        "                 separator=\";\", random_state=0,\n",
        "                transform=None, output_length=None,\n",
        "                return_pre_token_lists=True,\n",
        "                leading_zeros=True,\n",
        "                return_pre_tokens=False):\n",
        "\n",
        "        self.Zn = Zn\n",
        "        self.n_operands = n_operands\n",
        "        self.n_samples = n_samples\n",
        "        self.n_items_per_sample = n_items_per_sample\n",
        "        self.operators = operators\n",
        "        self.separator = separator\n",
        "        self.transform = transform\n",
        "        self.output_length = output_length\n",
        "        self.return_pre_token_lists = return_pre_token_lists\n",
        "        self.leading_zeros = leading_zeros\n",
        "        self.return_pre_tokens = return_pre_tokens\n",
        "\n",
        "        self.expression_tree_dataset = ExpressionTreeDataset(n_operands=n_operands,\n",
        "                                                             n_samples=n_samples,\n",
        "                                                             operators=operators,\n",
        "                                                             n_expressions_per_sample=n_items_per_sample,\n",
        "                                                             random_state=random_state)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.expression_tree_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        trees = self.expression_tree_dataset[idx]\n",
        "        rng = np.random.RandomState(self.expression_tree_dataset.seeds[idx])\n",
        "\n",
        "        numberss = rng.randint(0, self.Zn, (len(trees), self.n_operands))\n",
        "\n",
        "        ndigits = int(np.ceil(np.log10(self.Zn)))\n",
        "        leading = \"0\" if self.leading_zeros else \"\"\n",
        "        format_string = f\"{{:{leading}{ndigits}d}}\"\n",
        "        node_rendering = lambda a, b, op: f\"({a}{op}{b})\"\n",
        "        node_rendering_for_token_list = lambda a, b, op: ['('] + a + [op] + b + [')']\n",
        "\n",
        "        rendered_expressions = []\n",
        "        rendered_expressions_for_token_list = []\n",
        "        # rendered_for_evals = []\n",
        "        evaled_expressions = []\n",
        "        for tree, numbers in zip(trees, numberss):\n",
        "            leaf_rendering = lambda i: format_string.format(numbers[i])\n",
        "            leaf_rendering_for_token_list = lambda i: [numbers[i]]\n",
        "\n",
        "            rendered_expression = render_expression_from_tree(tree,\n",
        "                                    render_expr=node_rendering,\n",
        "                                    render_leaf=leaf_rendering)\n",
        "            rendered_expressions.append(rendered_expression)\n",
        "\n",
        "            rendered_expression_for_token_list = render_expression_from_tree(tree,\n",
        "                                    render_expr=node_rendering_for_token_list,\n",
        "                                    render_leaf=leaf_rendering_for_token_list)\n",
        "            rendered_expressions_for_token_list.append(rendered_expression_for_token_list)\n",
        "\n",
        "            rendered_for_eval = render_expression_from_tree(tree,\n",
        "                                                            render_expr=node_rendering,\n",
        "                                                            render_leaf=lambda i:f\"{numbers[i]:d}\")\n",
        "            # rendered_for_evals.append(rendered_for_eval)\n",
        "            evaled_expressions.append(eval(rendered_for_eval) % self.Zn)\n",
        "\n",
        "        eqn_string = f\"{{}}={{:0{ndigits}d}}\"\n",
        "        equations = [eqn_string.format(r, e) for r, e in zip(rendered_expressions, evaled_expressions)]\n",
        "        # equations = [f\"{r}={e}\" for r, e in zip(rendered_for_evals, evaled_expressions)]\n",
        "\n",
        "        token_list_equations = [r + [\"=\", e]\n",
        "                            for r, e in zip(rendered_expressions_for_token_list, evaled_expressions)]\n",
        "\n",
        "        joined_string = self.separator.join(equations)\n",
        "\n",
        "        if self.return_pre_token_lists:\n",
        "            return joined_string, token_list_equations\n",
        "        return joined_string\n",
        "\n",
        "\n",
        "\n",
        "class FloatArithmeticDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, distribution='standard_normal',\n",
        "                  uniform_distribution_range=2,\n",
        "                    n_decimals=None,\n",
        "                 n_operands=2, n_samples=1000000,\n",
        "                 n_items_per_sample=1, operators=\"+-*\",\n",
        "                 separator=\"\", random_state=0,\n",
        "                transform=None, output_length=None,\n",
        "                return_pre_token_lists=True,\n",
        "                protect_division_by_zero=True):\n",
        "\n",
        "        self.distribution = distribution\n",
        "        self.uniform_distribution_range = uniform_distribution_range\n",
        "        self.n_decimals = n_decimals\n",
        "        self.n_operands = n_operands\n",
        "        self.n_samples = n_samples\n",
        "        self.n_items_per_sample = n_items_per_sample\n",
        "        self.operators = operators\n",
        "        self.separator = separator\n",
        "        self.transform = transform\n",
        "        self.output_length = output_length\n",
        "        self.return_pre_token_lists = return_pre_token_lists\n",
        "        self.protect_division_by_zero = protect_division_by_zero\n",
        "\n",
        "        self.random_state = random_state\n",
        "\n",
        "        self.expression_tree_dataset = ExpressionTreeDataset(n_operands=n_operands,\n",
        "                                                             n_samples=n_samples,\n",
        "                                                             operators=operators,\n",
        "                                                             n_expressions_per_sample=n_items_per_sample,\n",
        "                                                             random_state=random_state)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.expression_tree_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        trees = self.expression_tree_dataset[idx]\n",
        "        rng = np.random.RandomState(self.expression_tree_dataset.seeds[idx])\n",
        "        if self.distribution == 'standard_normal':\n",
        "            numberss = rng.randn(len(trees), self.n_operands)\n",
        "        elif self.distribution == \"uniform\":\n",
        "            numberss = rng.rand(len(trees), self.n_operands) * self.uniform_distribution_range - self.uniform_distribution_range/2\n",
        "        else:\n",
        "            raise NotImplementedError(\"Only implemented 'standard_normal' so far\")\n",
        "\n",
        "        ndigits = f\"1.{self.n_decimals}\" if self.n_decimals is not None else \"\"\n",
        "        format_string = f\"{{:{ndigits}f}}\"\n",
        "        node_rendering = lambda a, b, op: f\"({a}{op}{b})\"\n",
        "        node_rendering_for_token_list = lambda a, b, op: ['('] + a + [op] + b + [')']\n",
        "\n",
        "        rendered_expressions = []\n",
        "        rendered_expressions_for_token_list = []\n",
        "        evaled_expressions = []\n",
        "        for tree, numbers in zip(trees, numberss):\n",
        "            leaf_rendering = lambda i: format_string.format(numbers[i])\n",
        "            leaf_rendering_for_token_list = lambda i: [numbers[i] if self.n_decimals is None\n",
        "                                                            else np.round(numbers[i], self.n_decimals)]\n",
        "\n",
        "            rendered_expression = render_expression_from_tree(tree,\n",
        "                                    render_expr=node_rendering,\n",
        "                                    render_leaf=leaf_rendering)\n",
        "            rendered_expressions.append(rendered_expression)\n",
        "\n",
        "            rendered_expression_for_token_list = render_expression_from_tree(tree,\n",
        "                                    render_expr=node_rendering_for_token_list,\n",
        "                                    render_leaf=leaf_rendering_for_token_list)\n",
        "            rendered_expressions_for_token_list.append(rendered_expression_for_token_list)\n",
        "\n",
        "            rendered_for_eval = render_expression_from_tree(tree,\n",
        "                                                            render_expr=node_rendering,\n",
        "                                                            #render_leaf=lambda i:f\"{numbers[i]:f}\")\n",
        "                                                            render_leaf=lambda i: format_string.format(numbers[i]))\n",
        "            try:\n",
        "                evaled_expression = eval(rendered_for_eval)\n",
        "            except ZeroDivisionError:\n",
        "                if self.protect_division_by_zero:\n",
        "                    import warnings\n",
        "                    warnings.warn(f\"Found division by 0 at sample {idx}, replacing with the sample at index {idx + 1}\")\n",
        "                    return self[idx + 1]\n",
        "                else:\n",
        "                    raise\n",
        "\n",
        "            evaled_expressions.append(evaled_expression)\n",
        "\n",
        "        eqn_string = f\"{{}}={{:{ndigits}f}}\"\n",
        "        equations = [eqn_string.format(r, e) for r, e in zip(rendered_expressions, evaled_expressions)]\n",
        "\n",
        "        token_list_equations = [r + [\"=\", e]\n",
        "                            for r, e in zip(rendered_expressions_for_token_list, evaled_expressions)]\n",
        "\n",
        "\n",
        "        if self.return_pre_token_lists:\n",
        "            if self.transform is not None:\n",
        "                return self.transform(token_list_equations)\n",
        "            return token_list_equations\n",
        "\n",
        "        joined_string = self.separator.join(equations)\n",
        "        return joined_string\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j45N7szA0FJ0"
      },
      "outputs": [],
      "source": [
        "fad_train = FloatArithmeticDataset(n_operands=4, n_decimals=4, n_samples=5_000_000, distribution='uniform')\n",
        "fad_val = FloatArithmeticDataset(n_operands=4, n_decimals=4, n_samples=300_000, distribution='uniform')\n",
        "fad_test = FloatArithmeticDataset(n_operands=4, n_decimals=4, n_samples=300_000, distribution='uniform')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OWklSR-l0clH"
      },
      "outputs": [],
      "source": [
        "def save_dataset_to_file(dataset, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        for i in range(len(dataset)):\n",
        "            sample = dataset[i][0]\n",
        "            sample_str = ''.join(map(str, sample))  # Convert tensor to a space-separated string\n",
        "            f.write(sample_str + '\\n')\n",
        "\n",
        "save_dataset_to_file(fad_train, './data/fad_4_op_uniform_10/train')\n",
        "save_dataset_to_file(fad_val, './data/fad_4_op_uniform_10/val')\n",
        "save_dataset_to_file(fad_test, './data/fad_4_op_uniform_10/test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "dataset_path = \"./data/tokenized_fad_4/tokenized_ds_xval\"\n",
        "tokenized_ds = DatasetDict.load_from_disk(dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [18, 18, 3, 3, 20, 17, 18, 3, 17, 3, 20, 20, 19, 3],\n",
              " 'numbers': [1.0,\n",
              "  1.0,\n",
              "  -0.737,\n",
              "  -3.146,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.223,\n",
              "  1.0,\n",
              "  -0.405,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  -3.064]}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_ds[\"train\"][8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d44be2c8c3eb4bb89a25f38204f67fb2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f39eb6c0d05b473a8305ccf777f0d4db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a5a433740c44793afe610915fe80b01",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "text_ds = DatasetDict.from_text(\"./data/fad/train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': '(0.63-((-0.158+0.426)--0.492))=-0.13'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_ds[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xval.make_tokenizer import make_tokenizer\n",
        "\n",
        "make_tokenizer(\n",
        "    encoding=\"xval\",\n",
        "    save_file=tokenizer_path, \n",
        "    efficient_json=True, \n",
        "    sample_keys=sample_keys\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
