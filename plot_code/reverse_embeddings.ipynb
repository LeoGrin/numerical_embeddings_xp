{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1  = pd.read_csv(\"results_summarised/all_results_sort_06_04_full.csv\")\n",
    "df1[\"file\"] = \"all_results_sort_06_04_full.csv\"\n",
    "df2  = pd.read_csv(\"results_summarised/all_new_results_sort_14_05_full.csv\")\n",
    "df2[\"file\"] = \"all_new_results_sort_14_05_full.csv\"\n",
    "df3  = pd.read_csv(\"results_summarised/sort_different_depth_17_05_2.csv\")\n",
    "df3[\"file\"] = \"sort_different_depth_17_05_2.csv\"\n",
    "df = pd.concat([df1, df2, df3])\n",
    "# df = pd.concat([pd.read_csv(\"results_summarised/all_results_sort_06_04_full.csv\"),\n",
    "#                 pd.read_csv(\"results_summarised/all_new_results_sort_14_05_full.csv\"),\n",
    "#                 pd.read_csv(\"results_summarised/sort_different_depth_17_05_2.csv\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame and 'test_loss_list' is the column you're working with.\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        # Try to evaluate the row as a literal.\n",
    "        pd.eval(row['test_loss_list'])\n",
    "    except Exception as e:\n",
    "        # If an exception occurs, print the index and the value that failed\n",
    "        print(f\"Failed at index {index}: {row['test_loss_list']}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        # Optionally, break after the first failure\n",
    "        #break\n",
    "import ast\n",
    "# replace \"nan\" with None\n",
    "df['test_loss_list'] = df['test_loss_list'].str.replace('nan', 'None')\n",
    "df['loss_list'] = df['loss_list'].str.replace('nan', 'None')\n",
    "#df['test_loss_list'] = df['test_loss_list'].apply(lambda s: pd.eval(s))\n",
    "#df['loss_list'] = df['loss_list'].apply(lambda s: pd.eval(s))\n",
    "\n",
    "df['test_loss_list'] = df['test_loss_list'].apply(ast.literal_eval)\n",
    "df['loss_list'] = df['loss_list'].apply(ast.literal_eval)\n",
    "\n",
    "# Create a new DataFrame for Plotly Express\n",
    "rows_list = []\n",
    "for _, row in df.iterrows():\n",
    "    params = row.to_dict()\n",
    "    test_loss_list = params.pop('test_loss_list')\n",
    "    loss_list = params.pop('loss_list')\n",
    "    for epoch, loss in enumerate(test_loss_list):\n",
    "        params['epoch'] = epoch\n",
    "        params['test_loss'] = loss\n",
    "        params['train_loss'] = loss_list[epoch]\n",
    "        rows_list.append(params.copy())  # Use .copy() to avoid modifying the original params\n",
    "\n",
    "# Create a new DataFrame\n",
    "df_long = pd.DataFrame(rows_list)\n",
    "df_long = df_long.fillna(\"missing\")\n",
    "# make embedding_function \"gaussian\" or \"none\"\n",
    "\n",
    "def convert_embedding(name):\n",
    "    if \"GaussianEmbedder\" in name:\n",
    "        return \"gaussian\"\n",
    "    elif \"PositionalEncoding\" in name:\n",
    "        return \"positional\"\n",
    "    elif \"RandomFeature\" in name:\n",
    "        return \"fourier\"\n",
    "    else:\n",
    "        return \"none\"\n",
    "df_long[\"embedding\"] = df_long[\"embedding_function\"].apply(convert_embedding)\n",
    "\n",
    "# remove Unnamed: 0\n",
    "df_long = df_long.drop(columns=[\"Unnamed: 0\", \"job_id\"])\n",
    "#df_long[(df_long[\"embedding\"] == \"fourier\") & (df_long[\"epoch\"] == 100) & (df_long[\"kernel\"] == \"gaussian\") & (df_long[\"n_layer\"] == 8) & (df_long[\"holes\"] == \"{}\") & (df_long[\"length_scale\"] == 1.)][[\"iter\", \"train_loss\", \"test_loss\"]].sort_values(\"iter\")\n",
    "# average over iter\n",
    "# fill na with \"missing\"\n",
    "df_long = df_long.fillna(\"missing\")\n",
    "# convert all to string except test_loss\n",
    "# removing rows where test_loss is missing\n",
    "df_long = df_long[df_long[\"test_loss\"] != \"missing\"]\n",
    "# same for epoch\n",
    "df_long = df_long[df_long[\"epoch\"] != \"missing\"]\n",
    "df_long = df_long.applymap(str)\n",
    "df_long[\"test_loss\"] = df_long[\"test_loss\"].astype(float)\n",
    "df_long[\"epoch\"] = df_long[\"epoch\"].astype(int)\n",
    "df_long[\"train_loss\"] = df_long[\"train_loss\"].astype(float)\n",
    "cols = list(df_long.columns)\n",
    "# # remove iter and test_loss\n",
    "cols.remove(\"iter\")\n",
    "cols.remove(\"test_loss\")\n",
    "cols.remove(\"train_loss\")\n",
    "# check that we have 5 rows for each combination of the other columns\n",
    "#assert all(df_long.groupby(cols).size() > 5)\n",
    "#show groups with less than 5\n",
    "#df_long.groupby(cols).size().sort_values()\n",
    "#df_long = df_long.groupby(cols)[\"test_loss\"].mean().reset_index()\n",
    "# #add a column for the number of rows and the mean test_loss\n",
    "# df_long = df_long.groupby(cols).agg(\n",
    "#     test_loss=('test_loss', 'median'),  # Calculate mean test_loss\n",
    "#     train_loss=('train_loss', 'median'),  # Calculate mean test_loss\n",
    "#     count=('test_loss', 'count'),           # Count the number of entries used for the mean\n",
    "#     std=('test_loss', 'std')\n",
    "# ).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_revert = df_long[df_long[\"revert_embedding\"] == \"True\"]\n",
    "df_revert = df_long[df_long[\"attention_only\"] == \"False\"]\n",
    "print(np.unique(df_revert[\"n_layer\"], return_counts=True))\n",
    "print(np.unique(df_revert[\"n_embd\"], return_counts=True))\n",
    "print(np.unique(df_revert[\"n_head\"], return_counts=True))\n",
    "print(np.unique(df_revert[\"parametrize_std\"], return_counts=True))\n",
    "print(np.unique(df_revert[\"n_epochs\"], return_counts=True))\n",
    "print(np.unique(df_revert[\"bandwidth\"], return_counts=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "filter_dic = {\n",
    "    #\"revert_embedding\": \"True\"\n",
    "    \"attention_only\": \"True\",\n",
    "    \"n_layer\": 8,\n",
    "    \"n_embd\": 128,\n",
    "    \"parametrize_std\": \"False\",\n",
    "    #\"read_in_bias\": \"False\",\n",
    "    #\"bandwidth\": \"[0.1171875]\",\n",
    "    \"embedding\": \"gaussian\",\n",
    "\n",
    "    \n",
    "}\n",
    "df_plot = df_long.copy()\n",
    "for key, value in filter_dic.items():\n",
    "    df_plot = df_plot[np.isin(df_plot[key], [value, \"missing\"])]\n",
    "\n",
    "# remove embedding fourier\n",
    "df_plot = df_plot[df_plot[\"embedding\"] != \"fourier\"]\n",
    "df_plot = df_plot[np.isin(df_plot[\"bandwidth\"], ['[0.1171875]', '[0.5859375]'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = list(df_plot.columns.difference([\"test_loss\", \"train_loss\", \"iter\"]))\n",
    "df_plot = df_plot.groupby(group_cols).agg(\n",
    "    test_loss=('test_loss', 'median'),  # Calculate mean test_loss\n",
    "    train_loss=('train_loss', 'median'),  # Calculate mean test_loss\n",
    "    count=('test_loss', 'count'),           # Count the number of entries used for the mean\n",
    "    std=('test_loss', 'std')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show remaining non-unique columns\n",
    "for col in group_cols:\n",
    "    if len(np.unique(df_plot[col])) > 1:\n",
    "        # check if missing is in the unique values\n",
    "        if \"missing\" in np.unique(df_plot[col]) and len(np.unique(df_plot[col])) == 2:\n",
    "            continue\n",
    "        elif len(np.unique(df_plot[col])) < 100:\n",
    "            print(col, np.unique(df_plot[col], return_counts=True))\n",
    "        else:\n",
    "            print(col, len(np.unique(df_plot[col])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot[\"bandwidth\"] = df_plot[\"bandwidth\"].apply(lambda x: x.replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\"))\n",
    "# rename bandwidth\n",
    "df_plot[\"bandwidth\"] = df_plot[\"bandwidth\"].apply(lambda x: f\"{float(x):.2f}\")\n",
    "df_plot[\"Length scale\"] = df_plot[\"bandwidth\"]\n",
    "df_plot[\"Revert Embedding\"] = df_plot[\"revert_embedding\"]\n",
    "df_plot[\"Epoch\"] = df_plot[\"epoch\"]\n",
    "\n",
    "# make a random plot\n",
    "fig = px.scatter(df_plot,\n",
    "           color = \"Revert Embedding\",\n",
    "           facet_col=\"Length scale\",\n",
    "           x=\"Epoch\",\n",
    "              y=\"test_loss\",\n",
    "              hover_data=\"file\",\n",
    "              log_y=True,\n",
    ")\n",
    "fig.write_image(\"plots/random.pdf\")\n",
    "\n",
    "fig = px.scatter(df_plot,\n",
    "           color = \"Revert Embedding\",\n",
    "           facet_col=\"Length scale\",\n",
    "           x=\"Epoch\",\n",
    "              y=\"test_loss\",\n",
    "              hover_data=\"file\",\n",
    "              log_y=True,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "      #title=\"Test loss over epochs\",\n",
    "      #xaxis_title=\"Epoch\",\n",
    "      yaxis_title=\"Test loss\",\n",
    "      # bigger font\n",
    "      font=dict(\n",
    "          size=18,\n",
    "      ),\n",
    "   )\n",
    "\n",
    "# save as pdf\n",
    "fig.write_image(\"plots/revert_embeddings.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "filter_dic = {\n",
    "    #\"revert_embedding\": \"True\"\n",
    "    \"attention_only\": \"False\",\n",
    "    \"n_layer\": 4,\n",
    "    \"n_embd\": 256,\n",
    "    #\"parametrize_std\": \"False\",\n",
    "    #\"read_in_bias\": \"False\",\n",
    "    #\"bandwidth\": \"[0.1171875]\",\n",
    "    #\"embedding\": \"gaussian\",\n",
    "\n",
    "    \n",
    "}\n",
    "df_plot = df_long.copy()\n",
    "for key, value in filter_dic.items():\n",
    "    df_plot = df_plot[np.isin(df_plot[key], [value, \"missing\"])]\n",
    "\n",
    "# remove embedding fourier\n",
    "#df_plot = df_plot[df_plot[\"embedding\"] != \"fourier\"]\n",
    "#df_plot = df_plot[np.isin(df_plot[\"bandwidth\"], ['[0.1171875]', '[0.5859375]'])]\n",
    "group_cols = list(df_plot.columns.difference([\"test_loss\", \"train_loss\", \"iter\"]))\n",
    "df_plot = df_plot.groupby(group_cols).agg(\n",
    "    test_loss=('test_loss', 'median'),  # Calculate mean test_loss\n",
    "    train_loss=('train_loss', 'median'),  # Calculate mean test_loss\n",
    "    count=('test_loss', 'count'),           # Count the number of entries used for the mean\n",
    "    std=('test_loss', 'std')\n",
    ").reset_index()\n",
    "# show remaining non-unique columns\n",
    "for col in group_cols:\n",
    "    if len(np.unique(df_plot[col])) > 1:\n",
    "        # check if missing is in the unique values\n",
    "        if \"missing\" in np.unique(df_plot[col]) and len(np.unique(df_plot[col])) == 2:\n",
    "            continue\n",
    "        elif len(np.unique(df_plot[col])) < 100:\n",
    "            print(col, np.unique(df_plot[col], return_counts=True))\n",
    "        else:\n",
    "            print(col, len(np.unique(df_plot[col])))\n",
    "#df_plot[\"bandwidth\"] = df_plot[\"bandwidth\"].apply(lambda x: x.replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\"))\n",
    "# rename bandwidth\n",
    "#df_plot[\"bandwidth\"] = df_plot[\"bandwidth\"].apply(lambda x: f\"{float(x):.2f}\")\n",
    "df_plot[\"Length scale\"] = df_plot[\"bandwidth\"]\n",
    "df_plot[\"Revert Embedding\"] = df_plot[\"revert_embedding\"]\n",
    "df_plot[\"Epoch\"] = df_plot[\"epoch\"]\n",
    "\n",
    "# make a random plot\n",
    "# fig = px.scatter(df_plot,\n",
    "#            color = \"Revert Embedding\",\n",
    "#            facet_col=\"Length scale\",\n",
    "#            x=\"Epoch\",\n",
    "#               y=\"test_loss\",\n",
    "#               hover_data=\"file\",\n",
    "#               log_y=True,\n",
    "# )\n",
    "#fig.write_image(\"plots/random.pdf\")\n",
    "\n",
    "fig = px.scatter(df_plot,\n",
    "           color = \"embedding\",\n",
    "           facet_col=\"Length scale\",\n",
    "           x=\"Epoch\",\n",
    "              y=\"test_loss\",\n",
    "              hover_data=\"file\",\n",
    "              log_y=True,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "      #title=\"Test loss over epochs\",\n",
    "      #xaxis_title=\"Epoch\",\n",
    "      yaxis_title=\"Test loss\",\n",
    "      # bigger font\n",
    "      font=dict(\n",
    "          size=18,\n",
    "      ),\n",
    "   )\n",
    "\n",
    "# save as pdf\n",
    "#fig.write_image(\"plots/revert_embeddings.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skrub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
