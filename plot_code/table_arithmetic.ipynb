{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# get all files starting with \"xval_\" in results\n",
    "#files = [f for f in os.listdir(\"results\") if os.path.isfile(os.path.join(\"results\", f)) and \"xval_2_5\" in f or \"xval_3_5\" in f and \"old\" not in f]\n",
    "files_6_op = [f for f in os.listdir(\"results\") if os.path.isfile(os.path.join(\"results\", f)) and (\"xval_fad_big_4_5\" in f or \"xval_fad_big_5_5\" in f) and (\"mlm\" not in f)]\n",
    "files_4_op = [f for f in os.listdir(\"results\") if os.path.isfile(os.path.join(\"results\", f)) and ((\"xval_fad_big_3_5\" in f) and (\"mlm\" not in f)) or (\"xval_fad_normal_8_5\" in f)]\n",
    "files_4_op_mlm = [f for f in os.listdir(\"results\") if os.path.isfile(os.path.join(\"results\", f)) and \"mlm_xval_fad_big_4_5\" in f]\n",
    "files_4_op_small_model = [f for f in os.listdir(\"results\") if os.path.isfile(os.path.join(\"results\", f)) and \"xval_fad_big_small_model_5_5\" in f]\n",
    "files_4_op_uniform = [f for f in os.listdir(\"results\") if os.path.isfile(os.path.join(\"results\", f)) and (\"xval_fad_big_small_model_6_5\" in f or \"xval_fad_big_small_model_7_5\" in f or \"xval_fad_uniform_7_5\" in f)]\n",
    "files_unknown = [f for f in os.listdir(\"results\") if os.path.isfile(os.path.join(\"results\", f)) and (\"xval_fad_uniform_3_20\" in f) or (\"xval_fad_uniform_small_lengthscale_21_5\" in f) or (\"xval_orbits_varying_lengthscale_21_5\" in f) or (\"xval_orbits_varying_lengthscale_20_5\" in f)]\n",
    "#files_unknown = [f for f in os.listdir(\"results\") if os.path.isfile(os.path.join(\"results\", f)) and (\"xval_orbits_varying_lengthscale_21_5\" in f) or (\"xval_orbits_varying_lengthscale_20_5\" in f)]\n",
    "files_varying_lengthscale = [f for f in os.listdir(\"results\") if os.path.isfile(os.path.join(\"results\", f)) and (\"xval_fad_normal_8_5\" in f) or (\"xval_fad_uniform_small_lengthscale_9_5\" in f) or (\"xval_fad_uniform_small_lengthscale_10_5\" in f) or (\"xval_fad_uniform_small_lengthscale_13_5\" in f) or (\"xval_fad_uniform_13_5\" in f) or (\"xval_fad_uniform_3_20\" in f)] + files_4_op_uniform + files_4_op\n",
    "files_different_depth = [f for f in os.listdir(\"results\") if os.path.isfile(os.path.join(\"results\", f)) and (\"xval_fad_uniform_13_5\" in f) or (\"xval_fad_uniform_14_5\" in f)]\n",
    "#files_4_op_uniform_3 = [f for f in os.listdir(\"results\") if os.path.isfile(os.path.join(\"results\", f)) and (\"xval_fad_uniform_7_5\" in f)]\n",
    "#files_4_op_uniform_jz = [f for f in os.listdir(\"results\") if os.path.isfile(os.path.join(\"results\", f)) and (\"xval_fad_big_6_5\" in f)]\n",
    "print(f\"Found {len(files_6_op)} files\")\n",
    "print(f\"Found {len(files_4_op)} files\")\n",
    "print(f\"Found {len(files_4_op_mlm)} files\")\n",
    "print(f\"Found {len(files_4_op_small_model)} files\")\n",
    "print(f\"Found {len(files_4_op_uniform)} files\")\n",
    "print(f\"Found {len(files_unknown)} files\")\n",
    "print(f\"Found {len(files_varying_lengthscale)} files\")\n",
    "print(f\"Found {len(files_different_depth)} files\")\n",
    "#print(f\"Found {len(files_4_op_uniform_3)} files\")\n",
    "\n",
    "#print(f\"Found {len(files_4_op_uniform_jz)} files\")\n",
    "\n",
    "# get all the results in a dataframe\n",
    "results = []\n",
    "for files, xp_name in zip([files_6_op, files_4_op, files_4_op_mlm, files_4_op_small_model, files_4_op_uniform, files_unknown, files_varying_lengthscale, files_different_depth],\n",
    "                           [\"6_op\", \"4_op\", \"4_op_mlm\", \"4_op_small_model\", \"4_op_uniform\", \"unknown\", \"varying_lengthscale\", \"different_depth\"]):\n",
    "    for file in files:\n",
    "        df = pd.read_csv(f\"results/{file}\")#.sample(10_000)\n",
    "        df[\"experiment\"] = xp_name\n",
    "        results.append(df)\n",
    "\n",
    "# concatenate the dataframes\n",
    "results = pd.concat(results)\n",
    "\n",
    "# remove missing columns\n",
    "results = results.dropna(axis=1, how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.fillna(\"missing\")\n",
    "# remove na epochs\n",
    "results = results[results[\"epoch\"] != \"missing\"]\n",
    "results = results.applymap(str)\n",
    "results[\"loss_mlm\"] = results[\"loss_mlm\"].astype(float)\n",
    "results[\"loss_num\"] = results[\"loss_num\"].astype(float)\n",
    "results[\"loss_total\"] = results[\"loss_total\"].astype(float)\n",
    "results[\"val_loss_mlm\"] = results[\"val_loss_mlm\"].astype(float)\n",
    "results[\"val_loss_num\"] = results[\"val_loss_num\"].astype(float)\n",
    "results[\"val_loss_total\"] = results[\"val_loss_total\"].astype(float)\n",
    "results[\"epoch\"] = results[\"epoch\"].astype(float).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "xp_fancy_names = {\n",
    "    \"6_op\": \"6 operands (predicting RHS)\",\n",
    "    \"4_op\": \"4 operands (predicting RHS)\",\n",
    "    \"4_op_mlm\": \"4 operands (predicting all tokens)\",\n",
    "    \"4_op_small_model\": \"4 operands (small model)\",\n",
    "    \"4_op_uniform\": \"4 operands (uniform)\",\n",
    "    \"unknown\": \"unknown\",\n",
    "    \"varying_lengthscale\": \"4 operands (varying lengthscale)\",\n",
    "}\n",
    "\n",
    "xp_name = \"unknown\"\n",
    "# filter\n",
    "#results_to_plot = results[results[\"experiment\"] == xp_name]\n",
    "results_to_plot = results.copy()\n",
    "\n",
    "# split dataset path to take only the part between /\n",
    "results_to_plot[\"dataset_path\"] = results_to_plot[\"dataset_path\"].str.split(\"/\").str[2]\n",
    "results_to_plot[\"collator\"] = results_to_plot[\"collator\"].str.split(\".\").str[0]\n",
    "\n",
    "results_to_plot[\"epoch\"] = results_to_plot[\"epoch\"].astype(float)\n",
    "filter_dic = {\n",
    "    \"num_layers\": \"6\",\n",
    "    \"reverse_numerical_embedding\": \"False\",\n",
    "    \"d_model\": \"384\",\n",
    "    \"dataset_path\": \"tokenized_fad_4_op_uniform\",\n",
    "    \"normalize_token_emb\": \"True\",\n",
    "    \"tokenizer_path\": \"./data/tokenized_fad_bigtokenizer_xval.json\",\n",
    "    #\"fourier_length_scale\": 0.1,\n",
    "    #\"fourier_length_scale\": \"0.0520833333333333\",\n",
    "    #\"gaussian_embedding_sd\": \"[0.052083333333333336]\",\n",
    "    #\"gaussian_embedding_sd\": 0.1,\n",
    "    #\"epoch\": \"30\",\n",
    "}\n",
    "\n",
    "# allow missing or value in filter_dic\n",
    "for key, value in filter_dic.items():\n",
    "    results_to_plot = results_to_plot[np.isin(results_to_plot[key], [value, \"missing\"])]\n",
    "\n",
    "# remove missing tokenizer_path\n",
    "results_to_plot = results_to_plot[results_to_plot[\"tokenizer_path\"] != \"missing\"]\n",
    "\n",
    "# if embedding is fourier, only keep \"fourier_length_scale\": \"0.0520833333333333\",\n",
    "results_to_plot = results_to_plot[(results_to_plot[\"embedding_type\"] != \"fourier\") | (results_to_plot[\"fourier_length_scale\"] == \"0.0052083333333333\")]\n",
    "# same for gaussian with \"[0.052083333333333336]\"\n",
    "#results_to_plot = results_to_plot[(results_to_plot[\"embedding_type\"] != \"gaussian\") | (results_to_plot[\"gaussian_embedding_sd\"] == \"[0.06510416666666667]\")]\n",
    "results_to_plot = results_to_plot[(results_to_plot[\"embedding_type\"] != \"gaussian\") | (results_to_plot[\"gaussian_embedding_sd\"] == \"[0.026041666666666668]\")]\n",
    "\n",
    "\n",
    "# remove duplicates\n",
    "results_to_plot = results_to_plot.drop_duplicates()\n",
    "\n",
    "# print model size\n",
    "print(\"num_layers\", results_to_plot[\"num_layers\"].unique())\n",
    "print(\"embedding_type\", results_to_plot[\"embedding_type\"].unique())\n",
    "print(\"d_model\", results_to_plot[\"d_model\"].unique())\n",
    "print(\"nhead\", results_to_plot[\"nhead\"].unique())\n",
    "print(\"dim_feedforward\", results_to_plot[\"dim_feedforward\"].unique())\n",
    "print(\"dataset_path\", results_to_plot[\"dataset_path\"].unique())\n",
    "print(\"fourier_length_scale\", results_to_plot[\"fourier_length_scale\"].unique())\n",
    "print(\"gaussian_embedding_sd\", results_to_plot[\"gaussian_embedding_sd\"].unique())\n",
    "# reverse_numerical_embedding\n",
    "print(\"reverse_numerical_embedding\", results_to_plot[\"reverse_numerical_embedding\"].unique())\n",
    "# collator\n",
    "print(\"collator\", results_to_plot[\"collator\"].unique())\n",
    "print(\"normalize_token_emb\", results_to_plot[\"normalize_token_emb\"].unique())\n",
    "print(\"nomalize_pos_enc\", results_to_plot[\"normalize_pos_enc\"].unique())\n",
    "\n",
    "\n",
    "# fix the palette for the colors\n",
    "palette = px.colors.qualitative.Plotly\n",
    "palette = np.array(palette)\n",
    "\n",
    "# remove gaussian embedding\n",
    "#results_to_plot = results_to_plot[results_to_plot[\"embedding_type\"] != \"gaussian\"]\n",
    "\n",
    "\n",
    "#group_cols = [\"epoch\", \"dim_feedforward\", \"dropout\", \"norm_first\", \"embedding_type\"]\n",
    "group_cols = results_to_plot.columns\n",
    "#group_cols.remove([\"loss_mlm\", \"loss_num\", \"loss_total\", \"iter\"])\n",
    "group_cols = [col for col in group_cols if col not in [\"loss_mlm\", \"loss_num\", \"loss_total\", \"val_loss_mlm\", \"val_loss_num\", \"val_loss_total\", \"epoch\"]]\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define the threshold\n",
    "threshold = 0.0005\n",
    "\n",
    "# Group by the specified columns and filter\n",
    "def min_epoch_below_threshold(group):\n",
    "    below_threshold = group[group['val_loss_total'] < threshold]\n",
    "    if not below_threshold.empty:\n",
    "        return below_threshold['epoch'].min()\n",
    "    return None  # Return None if no epochs are below the threshold\n",
    "\n",
    "# Group by the specified columns and apply the function\n",
    "result = results_to_plot.groupby(group_cols).apply(min_epoch_below_threshold).reset_index()\n",
    "\n",
    "# Rename the result column for clarity\n",
    "result.rename(columns={0: 'min_epoch_to_reach_threshold'}, inplace=True)\n",
    "\n",
    "# Display the result\n",
    "print(result)\n",
    "\n",
    "threshold = 0.001\n",
    "# Group by the specified columns and filter\n",
    "result2 = results_to_plot.groupby(group_cols).apply(lambda group: group[group['val_loss_total'] < threshold]['epoch'].min()).reset_index()\n",
    "\n",
    "# Rename the result column for clarity\n",
    "result2.rename(columns={0: 'min_epoch_to_reach_threshold'}, inplace=True)\n",
    "\n",
    "threshold = 0.0001\n",
    "# Group by the specified columns and filter\n",
    "result3 = results_to_plot.groupby(group_cols).apply(lambda group: group[group['val_loss_total'] < threshold]['epoch'].min()).reset_index()\n",
    "\n",
    "# Rename the result column for clarity\n",
    "result3.rename(columns={0: 'min_epoch_to_reach_threshold'}, inplace=True)\n",
    "\n",
    "# Display the result\n",
    "#fig = px.scatter(result, x='min_epoch_to_reach_threshold', y=\"embedding_type\", title='Minimum epoch to reach threshold', height=900, color='min_epoch_to_reach_threshold', facet_col='dataset_path',\n",
    "#                 hover_data=result.columns)\n",
    "#fig.show()\n",
    "#results_to_plot[results_to_plot[\"epoch\"] == 0.0]\n",
    "# combine the two results\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Ensure 'group_cols' are set correctly as per your earlier code\n",
    "group_cols = results_to_plot.columns\n",
    "group_cols = [col for col in group_cols if col not in [\"loss_mlm\", \"loss_num\", \"loss_total\", \"val_loss_mlm\", \"val_loss_num\", \"val_loss_total\", \"epoch\", \"min_epoch_to_reach_threshold\"]]\n",
    "\n",
    "# Merge results and results2 on group columns\n",
    "merged_results = pd.merge(result, result2, on=group_cols, how=\"left\", suffixes=('_threshold_0.0005', '_threshold_0.001'))\n",
    "merged_results = pd.merge(merged_results, result3, on=group_cols, how=\"left\")\n",
    "# rename min_epoch_to_reach_threshold to min_epoch_to_reach_threshold_threshold_0.0001\n",
    "merged_results.rename(columns={\"min_epoch_to_reach_threshold\": \"min_epoch_to_reach_threshold_threshold_0.0001\"}, inplace=True)\n",
    "# reorder columns\n",
    "merged_results = merged_results[group_cols + ['min_epoch_to_reach_threshold_threshold_0.001', 'min_epoch_to_reach_threshold_threshold_0.0005', 'min_epoch_to_reach_threshold_threshold_0.0001']]\n",
    "\n",
    "# Calculate standard deviation, min, and max for the epoch columns\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming merged_results is already defined and contains the necessary columns\n",
    "\n",
    "# Calculate mean, std, min, and max grouped by 'embedding_type'\n",
    "grouped_stats = merged_results.groupby('embedding_type').agg({\n",
    "    'min_epoch_to_reach_threshold_threshold_0.0005': ['mean', 'std', 'min', 'max', \"count\"],\n",
    "    'min_epoch_to_reach_threshold_threshold_0.001': ['mean', 'std', 'min', 'max'],\n",
    "    'min_epoch_to_reach_threshold_threshold_0.0001': ['mean', 'std', 'min', 'max'],\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the MultiIndex in columns\n",
    "grouped_stats.columns = ['_'.join(col).strip() if col[1] else col[0] for col in grouped_stats.columns.values]\n",
    "\n",
    "# Format columns as 'mean (min-max)'\n",
    "grouped_stats['Formatted_0.0005'] = grouped_stats.apply(\n",
    "    lambda row: f\"{row['min_epoch_to_reach_threshold_threshold_0.0005_mean']:.2f} ({row['min_epoch_to_reach_threshold_threshold_0.0005_min']}-{row['min_epoch_to_reach_threshold_threshold_0.0005_max']})\",\n",
    "    axis=1\n",
    ")\n",
    "grouped_stats['Formatted_0.001'] = grouped_stats.apply(\n",
    "    lambda row: f\"{row['min_epoch_to_reach_threshold_threshold_0.001_mean']:.2f} ({row['min_epoch_to_reach_threshold_threshold_0.001_min']}-{row['min_epoch_to_reach_threshold_threshold_0.001_max']})\",\n",
    "    axis=1\n",
    ")\n",
    "grouped_stats['Formatted_0.0001'] = grouped_stats.apply(\n",
    "    lambda row: f\"{row['min_epoch_to_reach_threshold_threshold_0.0001_mean']:.2f} ({row['min_epoch_to_reach_threshold_threshold_0.0001_min']}-{row['min_epoch_to_reach_threshold_threshold_0.0001_max']})\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Select the final columns for the table\n",
    "final_table = grouped_stats[['embedding_type', 'Formatted_0.001', 'Formatted_0.0005', 'Formatted_0.0001']]\n",
    "#final_table.columns = ['Group', 'Min Epoch (Threshold 0.0005)', 'Min Epoch (Threshold 0.001)']\n",
    "\n",
    "final_table.columns = ['Embedding', 'RMSE=0.001', 'RMSE=0.0005', 'RMSE=0.0001']\n",
    "# replace nans by \"Not reached\" (and don't show the min-max in this case)\n",
    "#final_table = final_table.fillna(\"Not reached\")\n",
    "final_table = final_table.replace(\"nan (nan-nan)\", \"Not reached\")\n",
    "\n",
    "# Convert DataFrame to LaTeX\n",
    "latex_table = final_table.to_latex(index=False, caption=\"Minimum Epochs to Reach Thresholds\")\n",
    "\n",
    "# Add these as new columns to the DataFrame\n",
    "# merged_results['Std Dev (Threshold 0.0005)'] = std_dev[0]\n",
    "# merged_results['Min-Max (Threshold 0.0005)'] = f\"{min_values[0]} - {max_values[0]}\"\n",
    "# merged_results['Std Dev (Threshold 0.001)'] = std_dev[1]\n",
    "# merged_results['Min-Max (Threshold 0.001)'] = f\"{min_values[1]} - {max_values[1]}\"\n",
    "\n",
    "\n",
    "# Select and rename columns for clarity\n",
    "#final_table = merged_results[['embedding_type', 'min_epoch_to_reach_threshold_threshold_0.0005', 'Std Dev (Threshold 0.0005)', 'Min-Max (Threshold 0.0005)', \n",
    "#                              'min_epoch_to_reach_threshold_threshold_0.001', 'Std Dev (Threshold 0.001)', 'Min-Max (Threshold 0.001)']]\n",
    "#final_table.columns = ['Group', 'Min Epoch (Threshold 0.0005)', 'Std Dev (0.0005)', 'Range (0.0005)', 'Min Epoch (Threshold 0.001)', 'Std Dev (0.001)', 'Range (0.001)']\n",
    "# Select and rename columns for clarity\n",
    "# final_table = merged_results[['embedding_type', 'min_epoch_to_reach_threshold_threshold_0.0005', 'min_epoch_to_reach_threshold_threshold_0.001']]\n",
    "# final_table.columns = ['Group', 'Min Epoch (Threshold 0.0005)', 'Min Epoch (Threshold 0.001)']\n",
    "\n",
    "# # # Convert DataFrame to LaTeX\n",
    "# latex_table = final_table.to_latex(index=False, caption=\"Minimum Epochs to Reach Thresholds\", label=\"tab:thresholds\", longtable=True)\n",
    "# print(latex_table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(results_to_plot[(results_to_plot[\"epoch\"] == 1.) & (results_to_plot[\"embedding_type\"] == \"gaussian\")][\"gaussian_embedding_sd\"], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(results_to_plot[(results_to_plot[\"epoch\"] == 1) & (results_to_plot[\"embedding_type\"] == \"fourier\")][\"fourier_length_scale\"], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_plot[(results_to_plot[\"iter\"] == \"2\") & (results_to_plot[\"epoch\"] == 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(results_to_plot, x=\"epoch\", y=\"val_loss_total\", color=\"embedding_type\", title=\"Validation loss per epoch\", facet_col=\"iter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skrub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
